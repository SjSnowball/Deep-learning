{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple\n",
    "import numpy.ma as ma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols,cols1,cols2,cols3=['sentence'], ['Vocabulary'],['slot'],['intent']\n",
    "df=pd.read_csv(\"train.csv\",names=cols)\n",
    "df1=pd.read_csv(\"vocab.csv\",names=cols1)\n",
    "df2=pd.read_csv(\"train_slots.csv\",names=cols2)\n",
    "df3=pd.read_csv(\"slots.csv\",names=cols3)\n",
    "df4=pd.read_csv(\"test.csv\",names=cols)\n",
    "intent=pd.read_csv(\"intents.csv\",names=cols3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      intent\n",
      "0               abbreviation\n",
      "1                   aircraft\n",
      "2  aircraft+flight+flight_no\n",
      "3                    airfare\n",
      "4             airfare+flight \n",
      "\n",
      "                                                 slot\n",
      "0  128 128 128 128 128 128 48 128 35 100 128 128 ...\n",
      "1    128 128 128 128 128 128 48 128 78 128 26 33 128\n",
      "2  128 128 128 128 45 108 128 48 110 128 128 35 1...\n",
      "3                       128 21 128 128 48 128 78 128\n",
      "4         128 66 119 128 128 48 128 78 21 38 103 128 \n",
      "\n",
      "   Vocabulary\n",
      "0         'd\n",
      "1      'hare\n",
      "2        'll\n",
      "3         'm\n",
      "4        're\n"
     ]
    }
   ],
   "source": [
    "print(intent[0:5],\"\\n\\n\",df2.head(),\"\\n\\n\",df1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=[]\n",
    "test=[]\n",
    "slot1=[]\n",
    "for i in range(df.shape[0]):\n",
    "    sntce=df.sentence[i].split()\n",
    "    slot=df2.slot[i].split()\n",
    "    array1=[]\n",
    "    array2=[]\n",
    "    \n",
    "    for j in range(len(sntce)):\n",
    "        array1.append(df1.Vocabulary[int(sntce[j])])\n",
    "    train.append(\" \".join(array1))\n",
    "    \n",
    "    \n",
    "    for k in range(len(slot)):\n",
    "        array2.append(df3.intent[int(slot[k])])\n",
    "    slot1.append(\" \".join(array2))\n",
    "    \n",
    "for i1 in range(df4.shape[0]):\n",
    "    sntce1=df4.sentence[i1].split()\n",
    "    array3=[]\n",
    "    for j1 in range(len(sntce1)):\n",
    "        array3.append(df1.Vocabulary[int(sntce1[j1])])\n",
    "    test.append(\" \".join(array3))\n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [t[:-1] for t in train]\n",
    "train = [t.split(\" \") for t in train]\n",
    "train = [t[1:-1] for t in train]\n",
    "\n",
    "test = [t[:-1] for t in test]\n",
    "test = [t.split(\" \") for t in test]\n",
    "test = [t[1:-1] for t in test]\n",
    "\n",
    "slot1 = [t[:-1] for t in slot1]\n",
    "slot1 = [t.split(\" \") for t in slot1]\n",
    "slot1 = [t[1:-1] for t in slot1]\n",
    "\n",
    "intent = [t for t in intent['intent']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abbreviation', 'aircraft', 'aircraft+flight+flight_no', 'airfare', 'airfare+flight'] \n",
      "\n",
      " [['i', 'want', 'to', 'fly', 'from', 'boston', 'at', '838', 'am', 'and', 'arrive', 'in', 'denver', 'at', '1110', 'in', 'the', 'morning'], ['what', 'flights', 'are', 'available', 'from', 'pittsburgh', 'to', 'baltimore', 'on', 'thursday', 'morning'], ['what', 'is', 'the', 'arrival', 'time', 'in', 'san', 'francisco', 'for', 'the', '755', 'am', 'flight', 'leaving', 'washington'], ['cheapest', 'airfare', 'from', 'tacoma', 'to', 'orlando'], ['round', 'trip', 'fares', 'from', 'pittsburgh', 'to', 'philadelphia', 'under', '1000', 'dollars']] \n",
      "\n",
      " [['O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-depart_time.time', 'I-depart_time.time', 'O', 'O', 'O', 'B-toloc.city_name', 'O', 'B-arrive_time.time', 'O', 'O', 'B-arrive_time.period_of_day'], ['O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name', 'O', 'B-depart_date.day_name', 'B-depart_time.period_of_day'], ['O', 'O', 'O', 'B-flight_time', 'I-flight_time', 'O', 'B-fromloc.city_name', 'I-fromloc.city_name', 'O', 'O', 'B-depart_time.time', 'I-depart_time.time', 'O', 'O', 'B-fromloc.city_name'], ['B-cost_relative', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name'], ['B-round_trip', 'I-round_trip', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name', 'B-cost_relative', 'B-fare_amount', 'I-fare_amount']]\n"
     ]
    }
   ],
   "source": [
    "print(intent[0:5],\"\\n\\n\",train[:5],\"\\n\\n\",slot1[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization with special tokens in seq2seq:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "vocab = set(flatten(train))\n",
    "slot_tag = set(flatten(slot1))\n",
    "intent_tag = set(intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'want', 'to', 'fly', 'from', 'boston', 'at', '838', 'am', 'and', 'arrive', 'in', 'denver', 'at', '1110', 'in', 'the', 'morning'], ['what', 'flights', 'are', 'available', 'from', 'pittsburgh', 'to', 'baltimore', 'on', 'thursday', 'morning'], ['what', 'is', 'the', 'arrival', 'time', 'in', 'san', 'francisco', 'for', 'the', '755', 'am', 'flight', 'leaving', 'washington'], ['cheapest', 'airfare', 'from', 'tacoma', 'to', 'orlando'], ['round', 'trip', 'fares', 'from', 'pittsburgh', 'to', 'philadelphia', 'under', '1000', 'dollars']] \n",
      "\n",
      " {'I-depart_date.today_relative', 'B-meal_code', 'I-airport_name', 'B-stoploc.city_name', 'B-flight_number', 'I-stoploc.city_name', 'B-depart_date.today_relative', 'I-toloc.city_name', 'B-return_date.date_relative', 'B-airline_code', 'I-fare_basis_code', 'B-fromloc.airport_code', 'I-flight_stop', 'B-arrive_date.today_relative', 'B-toloc.airport_name', 'B-return_date.today_relative', 'B-arrive_time.time_relative', 'I-flight_time', 'B-fromloc.airport_name', 'I-return_date.day_number', 'B-depart_time.time', 'B-depart_date.year', 'B-return_date.month_name', 'B-cost_relative', 'B-depart_date.date_relative', 'B-flight_days', 'B-return_time.period_of_day', 'B-toloc.state_code', 'I-fromloc.state_name', 'B-today_relative', 'B-stoploc.airport_name', 'B-arrive_date.month_name', 'I-depart_time.time_relative', 'I-fare_amount', 'B-time_relative', 'B-city_name', 'I-city_name', 'I-arrive_time.period_of_day', 'B-connect', 'B-flight_time', 'I-flight_mod', 'B-economy', 'I-restriction_code', 'B-aircraft_code', 'I-mod', 'I-toloc.state_name', 'B-airport_code', 'B-depart_time.start_time', 'I-arrive_date.day_number', 'B-transport_type', 'B-toloc.airport_code', 'I-arrive_time.end_time', 'I-economy', 'B-arrive_time.end_time', 'B-arrive_time.start_time', 'B-fromloc.state_name', 'B-depart_date.day_number', 'B-time', 'I-arrive_time.start_time', 'B-arrive_time.period_of_day', 'I-return_date.today_relative', 'B-depart_time.end_time', 'B-arrive_date.day_number', 'I-airline_name', 'I-cost_relative', 'I-toloc.airport_name', 'I-meal_description', 'I-arrive_time.time_relative', 'I-round_trip', 'B-airline_name', 'B-month_name', 'B-fromloc.state_code', 'I-today_relative', 'I-depart_date.day_name', 'B-day_number', 'I-time', 'B-toloc.city_name', 'B-fare_basis_code', 'B-arrive_date.date_relative', 'B-day_name', 'B-state_name', 'B-days_code', 'I-transport_type', 'I-depart_time.time', 'B-fromloc.city_name', 'B-flight_stop', 'B-mod', 'B-return_time.period_mod', 'B-arrive_time.period_mod', 'I-return_date.date_relative', 'B-meal', 'B-depart_time.period_of_day', 'B-stoploc.state_code', 'I-fromloc.airport_name', 'I-class_type', 'I-depart_time.end_time', 'B-depart_date.month_name', 'I-depart_time.period_of_day', 'B-round_trip', 'B-period_of_day', 'B-return_date.day_name', 'B-return_date.day_number', 'O', 'B-state_code', 'B-toloc.country_name', 'I-fromloc.city_name', 'B-depart_time.period_mod', 'B-or', 'B-arrive_date.day_name', 'B-meal_description', 'B-airport_name', 'B-class_type', 'I-depart_date.day_number', 'B-depart_time.time_relative', 'B-flight_mod', 'I-depart_time.start_time', 'I-meal_code', 'B-arrive_time.time', 'B-fare_amount', 'I-arrive_time.time', 'B-restriction_code', 'B-depart_date.day_name', 'B-toloc.state_name'} \n",
      "\n",
      " {'quantity', 'airfare', 'flight_no+airline', 'cheapest', 'ground_service+ground_fare', 'ground_service', 'flight_time', 'capacity', 'airfare+flight', 'airfare+flight_time', 'ground_fare', 'flight_no', 'flight+airline', 'airport', 'meal', 'airline', 'flight+airfare', 'aircraft', 'aircraft+flight+flight_no', 'distance', 'day_name', 'flight', 'airline+flight_no', 'abbreviation', 'restriction', 'city'}\n"
     ]
    }
   ],
   "source": [
    "print(train[:5],\"\\n\\n\", slot_tag, \"\\n\\n\" ,intent_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "LENGTH=50 # All the sequences in your batch should have the same length\n",
    "sin=[]\n",
    "sout=[]\n",
    "sin_test=[]\n",
    "sout_test=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(train)):\n",
    "    temp = train[i]\n",
    "    if len(temp)<LENGTH:\n",
    "        temp.append('<EOS>')\n",
    "        while len(temp)<LENGTH:\n",
    "            temp.append('<PAD>')\n",
    "    else:\n",
    "        temp = temp[:LENGTH]\n",
    "        temp[-1]='<EOS>'\n",
    "    sin.append(temp)\n",
    "    \n",
    "    temp = slot1[i]\n",
    "    if len(temp)<LENGTH:\n",
    "        while len(temp)<LENGTH:\n",
    "            temp.append('<PAD>')\n",
    "    else:\n",
    "        temp = temp[:LENGTH]\n",
    "        temp[-1]='<EOS>'\n",
    "    sout.append(temp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test)):\n",
    "    temp = test[i]\n",
    "    if len(temp)<LENGTH:\n",
    "        temp.append('<EOS>')\n",
    "        while len(temp)<LENGTH:\n",
    "            temp.append('<PAD>')\n",
    "    else:\n",
    "        temp = temp[:LENGTH]\n",
    "        temp[-1]='<EOS>'\n",
    "    sin_test.append(temp)\n",
    "    \n",
    "    temp = slot1[i]\n",
    "    if len(temp)<LENGTH:\n",
    "        while len(temp)<LENGTH:\n",
    "            temp.append('<PAD>')\n",
    "    else:\n",
    "        temp = temp[:LENGTH]\n",
    "        temp[-1]='<EOS>'\n",
    "    sout_test.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['i', 'want', 'to', 'fly', 'from', 'boston', 'at', '838', 'am', 'and', 'arrive', 'in', 'denver', 'at', '1110', 'in', 'the', 'morning', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>'], ['O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-depart_time.time', 'I-depart_time.time', 'O', 'O', 'O', 'B-toloc.city_name', 'O', 'B-arrive_time.time', 'O', 'O', 'B-arrive_time.period_of_day', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>'], 'abbreviation')\n"
     ]
    }
   ],
   "source": [
    "train = list(zip(sin,sout,intent))\n",
    "print(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['i', 'would', 'like', 'to', 'find', 'a', 'flight', 'from', 'charlotte', 'to', 'las', 'vegas', 'that', 'makes', 'a', 'stop', 'in', 'st.', 'louis', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>'], ['O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-depart_time.time', 'I-depart_time.time', 'O', 'O', 'O', 'B-toloc.city_name', 'O', 'B-arrive_time.time', 'O', 'O', 'B-arrive_time.period_of_day', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<EOS>'], 'abbreviation')\n"
     ]
    }
   ],
   "source": [
    "test = list(zip(sin_test,sout_test,intent))\n",
    "print(test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionaries of words and tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_from_training_data(data):\n",
    "    seq_in, seq_out, intent = list(zip(*data))\n",
    "    vocab = set(flatten(seq_in))\n",
    "    slot_tag = set(flatten(seq_out))\n",
    "    intent_tag = set(intent)\n",
    "   #word2index\n",
    "    word2index = {'<PAD>': 0, '<UNK>': 1, '<SOS>': 2, '<EOS>': 3}\n",
    "    for token in vocab:\n",
    "        if token not in word2index.keys():\n",
    "            word2index[token] = len(word2index)\n",
    "\n",
    "    # index2word\n",
    "    index2word = {v: k for k, v in word2index.items()}\n",
    "\n",
    "    # tag2index\n",
    "    tag2index = {'<PAD>': 0, '<UNK>': 1, \"O\": 2}\n",
    "    for tag in slot_tag:\n",
    "        if tag not in tag2index.keys():\n",
    "            tag2index[tag] = len(tag2index)\n",
    "\n",
    "    # index2tag\n",
    "    index2tag = {v: k for k, v in tag2index.items()}\n",
    "\n",
    "    # intent2index\n",
    "    intent2index = {'<UNK>': 0}\n",
    "    for ii in intent_tag:\n",
    "        if ii not in intent2index.keys():\n",
    "            intent2index[ii] = len(intent2index)\n",
    "\n",
    "    # index2intent\n",
    "    index2intent = {v: k for k, v in intent2index.items()}\n",
    "    return word2index, index2word, tag2index, index2tag, intent2index, index2intent\n",
    "\n",
    "\n",
    "word2index, index2word, slot2index, index2slot, intent2index, index2intent = get_info_from_training_data(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_seq2slot = lambda s, index2slot: [index2slot[i] for i in s]\n",
    "index_seq2word = lambda s, index2word: [index2word[i] for i in s]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<PAD>': 0, '<UNK>': 1, '<SOS>': 2, '<EOS>': 3, 'fares': 4, 'would': 5, 'columbus': 6, 'newark': 7, 'at': 8, 'after': 9, 'under': 10, 'arrive': 11, 'all': 12, '7': 13, 'bwi': 14, 'baltimore': 15, 'trip': 16, 'houston': 17, 'by': 18, 'angeles': 19, 'class': 20, 'want': 21, 'and': 22, 'show': 23, 'francisco': 24, 'information': 25, 'thursday': 26, 'cleveland': 27, 'first': 28, 'have': 29, 'airlines': 30, 'boston': 31, 'need': 32, 'cheapest': 33, '755': 34, 'washington': 35, 'united': 36, 'please': 37, 'los': 38, 'tomorrow': 39, 'round': 40, 'ground': 41, 'way': 42, 'philadelphia': 43, 'me': 44, 'is': 45, \"'d\": 46, 'pm': 47, 'fly': 48, 'dollars': 49, 'am': 50, 'flight': 51, 'for': 52, 'kind': 53, 'planes': 54, \"'s\": 55, 'denver': 56, '1110': 57, 'what': 58, 'a': 59, 'atlanta': 60, 'flights': 61, 'pittsburgh': 62, 'before': 63, 'list': 64, '838': 65, 'morning': 66, 'diego': 67, 'airport': 68, 'tacoma': 69, 'next': 70, 'aircraft': 71, 'kinds': 72, 'give': 73, 'leaving': 74, 'some': 75, 'book': 76, 'week': 77, 'like': 78, 'on': 79, 'in': 80, 'arrival': 81, 'of': 82, 'airfare': 83, 'to': 84, 'the': 85, 'i': 86, 'san': 87, 'available': 88, '6': 89, 'american': 90, '1000': 91, 'minneapolis': 92, 'dallas': 93, 'from': 94, 'orlando': 95, 'time': 96, 'tuesday': 97, 'are': 98, 'used': 99, 'ticket': 100, 'transportation': 101} \n",
      "\n",
      " {0: '<PAD>', 1: '<UNK>', 2: '<SOS>', 3: '<EOS>', 4: 'fares', 5: 'would', 6: 'columbus', 7: 'newark', 8: 'at', 9: 'after', 10: 'under', 11: 'arrive', 12: 'all', 13: '7', 14: 'bwi', 15: 'baltimore', 16: 'trip', 17: 'houston', 18: 'by', 19: 'angeles', 20: 'class', 21: 'want', 22: 'and', 23: 'show', 24: 'francisco', 25: 'information', 26: 'thursday', 27: 'cleveland', 28: 'first', 29: 'have', 30: 'airlines', 31: 'boston', 32: 'need', 33: 'cheapest', 34: '755', 35: 'washington', 36: 'united', 37: 'please', 38: 'los', 39: 'tomorrow', 40: 'round', 41: 'ground', 42: 'way', 43: 'philadelphia', 44: 'me', 45: 'is', 46: \"'d\", 47: 'pm', 48: 'fly', 49: 'dollars', 50: 'am', 51: 'flight', 52: 'for', 53: 'kind', 54: 'planes', 55: \"'s\", 56: 'denver', 57: '1110', 58: 'what', 59: 'a', 60: 'atlanta', 61: 'flights', 62: 'pittsburgh', 63: 'before', 64: 'list', 65: '838', 66: 'morning', 67: 'diego', 68: 'airport', 69: 'tacoma', 70: 'next', 71: 'aircraft', 72: 'kinds', 73: 'give', 74: 'leaving', 75: 'some', 76: 'book', 77: 'week', 78: 'like', 79: 'on', 80: 'in', 81: 'arrival', 82: 'of', 83: 'airfare', 84: 'to', 85: 'the', 86: 'i', 87: 'san', 88: 'available', 89: '6', 90: 'american', 91: '1000', 92: 'minneapolis', 93: 'dallas', 94: 'from', 95: 'orlando', 96: 'time', 97: 'tuesday', 98: 'are', 99: 'used', 100: 'ticket', 101: 'transportation'} \n",
      "\n",
      " {'<PAD>': 0, '<UNK>': 1, 'O': 2, 'B-flight_time': 3, 'B-depart_time.period_of_day': 4, 'B-stoploc.city_name': 5, 'I-class_type': 6, 'B-depart_date.today_relative': 7, 'I-toloc.city_name': 8, 'B-toloc.city_name': 9, 'B-round_trip': 10, 'I-fromloc.city_name': 11, 'I-flight_time': 12, 'B-depart_time.time': 13, 'I-depart_time.time': 14, 'B-toloc.airport_code': 15, 'B-class_type': 16, 'B-cost_relative': 17, 'B-depart_date.date_relative': 18, 'B-arrive_time.period_of_day': 19, 'B-depart_time.time_relative': 20, 'I-airline_name': 21, 'B-fromloc.city_name': 22, 'I-fare_amount': 23, 'B-arrive_time.time': 24, 'I-round_trip': 25, 'B-fare_amount': 26, 'B-city_name': 27, 'B-airline_name': 28, 'B-depart_date.day_name': 29}\n"
     ]
    }
   ],
   "source": [
    "print(word2index,\"\\n\\n\",index2word,\"\\n\\n\",slot2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map the sentences to a sequence of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_index(train, word2index, slot2index, intent2index):\n",
    "    new_train = []\n",
    "    for sin, sout, intent in train:\n",
    "        sin_ix = list(map(lambda i: word2index[i] if i in word2index else word2index[\"<UNK>\"],\n",
    "                          sin))\n",
    "        true_length = sin.index(\"<EOS>\")\n",
    "        sout_ix = list(map(lambda i: slot2index[i] if i in slot2index else slot2index[\"<UNK>\"],\n",
    "                           sout))\n",
    "        intent_ix = intent2index[intent] if intent in intent2index else intent2index[\"<UNK>\"]\n",
    "        new_train.append([sin_ix, true_length, sout_ix, intent_ix])\n",
    "    return new_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[86, 21, 84, 48, 94, 31, 8, 65, 50, 22, 11, 80, 56, 8, 57, 80, 85, 66, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 18, [2, 2, 2, 2, 2, 22, 2, 13, 14, 2, 2, 2, 9, 2, 24, 2, 2, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 24]\n"
     ]
    }
   ],
   "source": [
    "index_train = to_index(train, word2index, slot2index, intent2index)\n",
    "index_test = to_index(test, word2index, slot2index, intent2index)\n",
    "print(index_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# RNN- LSTM network with an embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_steps = 50\n",
    "embedding_size = 64\n",
    "hidden_size = 100\n",
    "n_layers = 2\n",
    "batch_size = 16\n",
    "vocab_size = 871\n",
    "slot_size = 122\n",
    "intent_size = 22\n",
    "epoch_num = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = tf.placeholder(tf.int32, [input_steps, batch_size],\n",
    "                                             name='encoder_inputs')\n",
    "# Actual length of each sentence input，apart from padding\n",
    "encoder_inputs_actual_length = tf.placeholder(tf.int32, [batch_size],\n",
    "                                                   name='encoder_inputs_actual_length')\n",
    "decoder_targets = tf.placeholder(tf.int32, [batch_size, input_steps],\n",
    "                                      name='decoder_targets')\n",
    "intent_targets = tf.placeholder(tf.int32, [batch_size],\n",
    "                                     name='intent_targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size],\n",
    "                                                        -0.1, 0.1), dtype=tf.float32, name=\"embedding\")\n",
    "\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_lookup_2/Identity:0' shape=(50, 16, 64) dtype=float32>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_inputs_embedded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the RNN-LSTM Cell model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_f_cell = LSTMCell(hidden_size)\n",
    "encoder_b_cell = LSTMCell(hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding seq_2_seq "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-159-80e35c78b95f>:6: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/sowmya/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/sowmya/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "(encoder_fw_outputs, encoder_bw_outputs), (encoder_fw_final_state, encoder_bw_final_state) = \\\n",
    "    tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_f_cell,\n",
    "                                    cell_bw=encoder_b_cell,\n",
    "                                    inputs=encoder_inputs_embedded,\n",
    "                                    sequence_length=encoder_inputs_actual_length,\n",
    "                                    dtype=tf.float32, time_major=True)\n",
    "encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2)\n",
    "\n",
    "encoder_final_state_c = tf.concat(\n",
    "    (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n",
    "\n",
    "encoder_final_state_h = tf.concat(\n",
    "    (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
    "\n",
    "encoder_final_state = LSTMStateTuple(\n",
    "    c=encoder_final_state_c,\n",
    "    h=encoder_final_state_h\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_outputs:  Tensor(\"concat:0\", shape=(50, 16, 200), dtype=float32)\n",
      "encoder_outputs[0]:  Tensor(\"strided_slice:0\", shape=(16, 200), dtype=float32)\n",
      "encoder_final_state_c:  Tensor(\"concat_1:0\", shape=(16, 200), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"encoder_outputs: \", encoder_outputs)\n",
    "print(\"encoder_outputs[0]: \", encoder_outputs[0])\n",
    "print(\"encoder_final_state_c: \", encoder_final_state_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_lengths = encoder_inputs_actual_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "slot_W = tf.Variable(tf.random_uniform([hidden_size * 2, slot_size], -1, 1),\n",
    "                             dtype=tf.float32, name=\"slot_W\")\n",
    "slot_b = tf.Variable(tf.zeros([slot_size]), dtype=tf.float32, name=\"slot_b\")\n",
    "intent_W = tf.Variable(tf.random_uniform([hidden_size * 2, intent_size], -0.1, 0.1),\n",
    "                               dtype=tf.float32, name=\"intent_W\")\n",
    "intent_b = tf.Variable(tf.zeros([intent_size]), dtype=tf.float32, name=\"intent_b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_logits = tf.add(tf.matmul(encoder_final_state_h, intent_W), intent_b)\n",
    "intent = tf.argmax(intent_logits, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_time_slice = tf.ones([batch_size], dtype=tf.int32, name='SOS') * 2\n",
    "sos_step_embedded = tf.nn.embedding_lookup(embeddings, sos_time_slice)\n",
    "pad_step_embedded = tf.zeros([batch_size, hidden_size * 2 + embedding_size],\n",
    "                             dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_fn():\n",
    "    initial_elements_finished = (0 >= decoder_lengths)  # all False at the initial step\n",
    "    initial_input = tf.concat((sos_step_embedded, encoder_outputs[0]), 1)\n",
    "    return initial_elements_finished, initial_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_fn(time, outputs, state):\n",
    "    prediction_id = tf.to_int32(tf.argmax(outputs, axis=1))\n",
    "    return prediction_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_inputs_fn(time, outputs, state, sample_ids):\n",
    "    # Output category on the previous time node，obtain embedding again as input to the next time node\n",
    "    pred_embedding = tf.nn.embedding_lookup(embeddings, sample_ids)\n",
    "    next_input = tf.concat((pred_embedding, encoder_outputs[time]), 1)\n",
    "    elements_finished = (time >= decoder_lengths)  # this operation produces boolean tensor of [batch_size]\n",
    "    all_finished = tf.reduce_all(elements_finished)  # -> boolean scalar\n",
    "    next_inputs = tf.cond(all_finished, lambda: pad_step_embedded, lambda: next_input)\n",
    "    next_state = state\n",
    "    return elements_finished, next_inputs, next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_helper = tf.contrib.seq2seq.CustomHelper(initial_fn, sample_fn, next_inputs_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(helper, scope, reuse=None):\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        memory = tf.transpose(encoder_outputs, [1, 0, 2])\n",
    "        attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "            num_units=hidden_size, memory=memory,\n",
    "            memory_sequence_length=encoder_inputs_actual_length)\n",
    "         #Loss and optimizer\n",
    "        cell = tf.contrib.rnn.LSTMCell(num_units=hidden_size * 2)\n",
    "        attn_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "            cell, attention_mechanism, attention_layer_size=hidden_size)\n",
    "        out_cell = tf.contrib.rnn.OutputProjectionWrapper(\n",
    "            attn_cell, slot_size, reuse=reuse\n",
    "        )\n",
    "        decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "            cell=out_cell, helper=helper,\n",
    "            initial_state=out_cell.zero_state(\n",
    "                dtype=tf.float32, batch_size=batch_size))\n",
    "        # initial_state=encoder_final_state)\n",
    "        final_outputs, final_state, final_sequence_lengths = tf.contrib.seq2seq.dynamic_decode(\n",
    "            decoder=decoder, output_time_major=True,\n",
    "            impute_finished=True, maximum_iterations=input_steps\n",
    "        )\n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs:  BasicDecoderOutput(rnn_output=<tf.Tensor 'decode/decoder/TensorArrayStack/TensorArrayGatherV3:0' shape=(?, 16, 122) dtype=float32>, sample_id=<tf.Tensor 'decode/decoder/TensorArrayStack_1/TensorArrayGatherV3:0' shape=(?, 16) dtype=int32>)\n",
      "outputs.rnn_output:  Tensor(\"decode/decoder/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, 16, 122), dtype=float32)\n",
      "outputs.sample_id:  Tensor(\"decode/decoder/TensorArrayStack_1/TensorArrayGatherV3:0\", shape=(?, 16), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "outputs = decode(my_helper, 'decode')\n",
    "print(\"outputs: \", outputs)\n",
    "print(\"outputs.rnn_output: \", outputs.rnn_output)\n",
    "print(\"outputs.sample_id: \", outputs.sample_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_targets_true_length:  Tensor(\"strided_slice_1:0\", shape=(?, 16), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "decoder_prediction = outputs.sample_id\n",
    "decoder_max_steps, decoder_batch_size, decoder_dim = tf.unstack(tf.shape(outputs.rnn_output))\n",
    "decoder_targets_time_majored = tf.transpose(decoder_targets, [1, 0])\n",
    "decoder_targets_true_length = decoder_targets_time_majored[:decoder_max_steps]\n",
    "print(\"decoder_targets_true_length: \", decoder_targets_true_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-172-b0ab6e62fc2c>:1: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "mask = tf.to_float(tf.not_equal(decoder_targets_true_length, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_slot = tf.contrib.seq2seq.sequence_loss(outputs.rnn_output, decoder_targets_true_length, weights=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-174-cfdfcefe6cfa>:4: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Loss and optimizer\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(intent_targets, depth=intent_size, dtype=tf.float32),\n",
    "    logits=intent_logits)\n",
    "loss_intent = tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_slot + loss_intent\n",
    "optimizer = tf.train.AdamOptimizer(name=\"a_optimizer\")\n",
    "grads, vars = zip(*optimizer.compute_gradients(loss))\n",
    "gradients, _ = tf.clip_by_global_norm(grads, 5)  # clip gradients\n",
    "train_op = optimizer.apply_gradients(zip(grads, vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(sess, mode, trarin_batch):\n",
    "    \"\"\" perform each batch\"\"\"\n",
    "    if mode not in ['train', 'test']:\n",
    "        print >> sys.stderr, 'mode is not supported'\n",
    "        sys.exit(1)\n",
    "    unziped = list(zip(*trarin_batch))\n",
    "    if mode == 'train':\n",
    "        output_feeds = [train_op, loss, decoder_prediction,\n",
    "                        intent]\n",
    "        feed_dict = {encoder_inputs: np.transpose(unziped[0], [1, 0]),\n",
    "                     encoder_inputs_actual_length: unziped[1],\n",
    "                     decoder_targets: unziped[2],\n",
    "                     intent_targets: unziped[3]}\n",
    "    if mode in ['test']:\n",
    "        output_feeds = [decoder_prediction, intent]\n",
    "        feed_dict = {encoder_inputs: np.transpose(unziped[0], [1, 0]),\n",
    "                     encoder_inputs_actual_length: unziped[1]}\n",
    "\n",
    "    results = sess.run(output_feeds, feed_dict=feed_dict)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBatch(batch_size, train_data):\n",
    "    random.shuffle(train_data)\n",
    "    sindex = 0\n",
    "    eindex = batch_size\n",
    "    while eindex < len(train_data):\n",
    "        batch = train_data[sindex:eindex]\n",
    "        temp = eindex\n",
    "        eindex = eindex + batch_size\n",
    "        sindex = temp\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def accuracy_score(true_data, pred_data, true_length=None):\n",
    "    true_data = np.array(true_data)\n",
    "    pred_data = np.array(pred_data)\n",
    "    assert true_data.shape == pred_data.shape\n",
    "    if true_length is not None:\n",
    "        val_num = np.sum(true_length)\n",
    "        assert val_num != 0\n",
    "        res = 0\n",
    "        for i in range(true_data.shape[0]):\n",
    "            res += np.sum(true_data[i, :true_length[i]] == pred_data[i, :true_length[i]])\n",
    "    else:\n",
    "        val_num = np.prod(true_data.shape)\n",
    "        assert val_num != 0\n",
    "        res = np.sum(true_data == pred_data)\n",
    "    res /= float(val_num)\n",
    "    return res\n",
    "def get_data_from_sequence_batch(true_batch, pred_batch, padding_token):\n",
    "    \"\"\"[[3,1,2,0,0,0],[5,2,1,4,0,0]] -> [3,1,2,5,2,1,4]\"\"\"\n",
    "    true_ma = ma.masked_equal(true_batch, padding_token)\n",
    "    pred_ma = ma.masked_array(pred_batch, true_ma.mask)\n",
    "    true_ma = true_ma.flatten()\n",
    "    pred_ma = pred_ma.flatten()\n",
    "    true_ma = true_ma[~true_ma.mask]\n",
    "    pred_ma = pred_ma[~pred_ma.mask]\n",
    "    return true_ma, pred_ma\n",
    "\n",
    "\n",
    "def f1_for_sequence_batch(true_batch, pred_batch, average=\"micro\", padding_token=0):\n",
    "    true, pred = get_data_from_sequence_batch(true_batch, pred_batch, padding_token)\n",
    "    labels = list(set(true))\n",
    "    return f1_score(true, pred, labels=labels, average=average)\n",
    "\n",
    "\n",
    "def accuracy_for_sequence_batch(true_batch, pred_batch, padding_token=0):\n",
    "    true, pred = get_data_from_sequence_batch(true_batch, pred_batch, padding_token)\n",
    "    return accuracy_score(true, pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicition and Slot Filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss at epoch 0, step 0: 6.999569\n",
      "[Epoch 0] Average train loss: 6.999568939208984\n",
      "Input Sentence        :  ['<UNK>', 'morning', 'i', 'would', 'like', 'to', 'fly', 'from', 'columbus', 'to', '<UNK>', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "Slot Truth            :  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<UNK>']\n",
      "Slot Prediction       :  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "Intent Truth          :  airline\n",
      "Intent Prediction     :  aircraft+flight+flight_no\n",
      "slot accuracy: 0.4975369458128079, intent accuracy: 0.0625\n",
      "F1 score for epoch 0: 0.6341463414634146\n",
      "\n",
      "Average train loss at epoch 1, step 0: 6.478260\n",
      "[Epoch 1] Average train loss: 6.478260040283203\n",
      "Input Sentence        :  ['on', 'next', '<UNK>', 'flight', 'from', '<UNK>', '<UNK>', 'to', '<UNK>', '<UNK>', 'arrive', 'in', '<UNK>', '<UNK>', '7', 'pm', '<UNK>', 'flight', 'on', 'thursday', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "Slot Truth            :  ['O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'I-fromloc.city_name', 'O', 'B-toloc.city_name', 'O', 'O', 'O', 'B-stoploc.city_name', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<UNK>']\n",
      "Slot Prediction       :  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Intent Truth          :  city\n",
      "Intent Prediction     :  aircraft+flight+flight_no\n",
      "slot accuracy: 0.5615763546798029, intent accuracy: 0.0625\n",
      "F1 score for epoch 1: 0.6327077747989276\n",
      "\n",
      "Average train loss at epoch 2, step 0: 6.111992\n",
      "[Epoch 2] Average train loss: 6.111992359161377\n",
      "Input Sentence        :  ['after', '<UNK>', 'pm', 'on', '<UNK>', '<UNK>', '<UNK>', 'i', 'would', 'like', 'to', 'fly', 'from', '<UNK>', '<UNK>', 'to', 'columbus', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "Slot Truth            :  ['O', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<UNK>']\n",
      "Slot Prediction       :  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', '<PAD>', '<PAD>', '<PAD>']\n",
      "Intent Truth          :  airport\n",
      "Intent Prediction     :  aircraft+flight+flight_no\n",
      "slot accuracy: 0.5497382198952879, intent accuracy: 0.0625\n",
      "F1 score for epoch 2: 0.6101694915254237\n",
      "\n",
      "Average train loss at epoch 3, step 0: 5.781352\n",
      "[Epoch 3] Average train loss: 5.781351566314697\n",
      "Input Sentence        :  ['<UNK>', '<UNK>', 'flights', 'from', '<UNK>', '<UNK>', '<UNK>', 'to', '<UNK>', '<UNK>', 'on', '<UNK>', '<UNK>', '<UNK>', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "Slot Truth            :  ['O', 'O', 'O', 'B-class_type', 'I-class_type', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<UNK>']\n",
      "Slot Prediction       :  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "Intent Truth          :  flight+airfare\n",
      "Intent Prediction     :  aircraft+flight+flight_no\n",
      "slot accuracy: 0.48792270531400966, intent accuracy: 0.0625\n",
      "F1 score for epoch 3: 0.6149253731343284\n",
      "\n",
      "Average train loss at epoch 4, step 0: 5.834069\n",
      "[Epoch 4] Average train loss: 5.83406925201416\n",
      "Input Sentence        :  ['show', 'flights', 'tuesday', '<UNK>', 'from', '<UNK>', 'to', '<UNK>', '<UNK>', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "Slot Truth            :  ['O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'I-fromloc.city_name', 'O', 'B-toloc.city_name', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<UNK>']\n",
      "Slot Prediction       :  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "Intent Truth          :  ground_service\n",
      "Intent Prediction     :  aircraft+flight+flight_no\n",
      "slot accuracy: 0.6201117318435754, intent accuracy: 0.0625\n",
      "F1 score for epoch 4: 0.6406685236768802\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch_num):\n",
    "    mean_loss = 0.0\n",
    "    train_loss = 0.0\n",
    "    for i, batch in enumerate(getBatch(batch_size, index_train)):\n",
    "        _, loss_v, decoder_prediction_v, intent_v = step(sess, \"train\", batch)\n",
    "        mean_loss += loss_v\n",
    "        train_loss += loss_v\n",
    "        if i % 30 == 0:\n",
    "            if i > 0:\n",
    "                mean_loss = mean_loss / 30.0\n",
    "            print('Average train loss at epoch %d, step %d: %f' % (epoch, i, mean_loss))\n",
    "            mean_loss = 0\n",
    "    train_loss /= (i + 1)\n",
    "    print(\"[Epoch {}] Average train loss: {}\".format(epoch, train_loss))\n",
    "\n",
    "    pred_slots = []\n",
    "    for j, batch in enumerate(getBatch(batch_size, index_test)):\n",
    "        decoder_prediction_v, intent_v = step(sess, \"test\", batch)\n",
    "        decoder_prediction_v = np.transpose(decoder_prediction_v, [1, 0])\n",
    "        if j == 0:\n",
    "            index = random.choice(range(len(batch)))\n",
    "            print(\"Input Sentence        : \", index_seq2word(batch[index][0], index2word))\n",
    "            print(\"Slot Truth            : \", index_seq2slot(batch[index][2], index2slot))\n",
    "            print(\"Slot Prediction       : \", index_seq2slot(decoder_prediction_v[index], index2slot))\n",
    "            print(\"Intent Truth          : \", index2intent[batch[index][3]])\n",
    "            print(\"Intent Prediction     : \", index2intent[intent_v[index]])\n",
    "        slot_pred_length = list(np.shape(decoder_prediction_v))[1]\n",
    "        pred_padded = np.lib.pad(decoder_prediction_v, ((0, 0), (0, input_steps-slot_pred_length)),\n",
    "                                 mode=\"constant\", constant_values=0)\n",
    "        pred_slots.append(pred_padded)\n",
    "        true_slot = np.array((list(zip(*batch))[2]))\n",
    "        true_length = np.array((list(zip(*batch))[1]))\n",
    "        true_slot = true_slot[:, :slot_pred_length]\n",
    "        slot_acc = accuracy_score(true_slot, decoder_prediction_v, true_length)\n",
    "        intent_acc = accuracy_score(list(zip(*batch))[3], intent_v)\n",
    "        print(\"slot accuracy: {}, intent accuracy: {}\".format(slot_acc, intent_acc))\n",
    "    pred_slots_a = np.vstack(pred_slots)\n",
    "    true_slots_a = np.array(list(zip(*index_test))[2])[:pred_slots_a.shape[0]]\n",
    "    print(\"F1 score for epoch {}: {}\\n\".format(epoch, f1_for_sequence_batch(true_slots_a, pred_slots_a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
